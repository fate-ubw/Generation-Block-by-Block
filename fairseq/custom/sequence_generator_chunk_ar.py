# Copyright (c) Facebook, Inc. and its affiliates.
# All rights reserved.
#
# This source code is licensed under the license found in the
# LICENSE file in the root directory of this source tree.
#

import math

import torch
import torch.nn.functional as F
from datetime import datetime
import numpy as np
from fairseq.utils import fill_with_neg_inf
import pdb

from fairseq.custom.sequence_generator import  SequenceGenerator


class ChunkArSequenceGenerator(SequenceGenerator):
    def __init__(self, tgt_dict, temperature=1.):
        super().__init__(tgt_dict,temperature)

    @torch.no_grad()
    def generate_completion(self, model, prefix_tokens, completion_length, topk, topp):
        """topk: <1 sampling, 1 greedy, >1 top-k sampling."""
        model.eval()
        pred_toks = []
        context = prefix_tokens
        states = {}
        valid_generated_length = 0
        # noteï¼švalid pad is not a variable in this test version, but will revise in the next versioin

        # First go over the context.
        for context_step in range(1, context.size(1)):
            _context = context[:, :context_step]
            _ = self._forward_one(model, _context, incremental_states=states, return_logits=True)
        while valid_generated_length < completion_length:
            logits, attn_t = self._forward_one(model, context, incremental_states=states, return_logits=True)
            pred_tok = self._topk_decode(logits, topk, topp)
            pred_toks.append(pred_tok)
            context = torch.cat((context, pred_tok), 1)
            if pred_tok.cpu().tolist()[0][0] not in [4,5]:
                valid_generated_length += 1

        pred_toks = torch.cat(pred_toks, 1)#concat all tensor which generated by model
        return pred_toks 


    def _forward_one(self, model, tokens, incremental_states=None, temperature=1., return_attn=False, return_logits=False, **decoder_kwargs):
        if incremental_states is not None:
            decoder_out = list(model.decoder(tokens, None, incremental_state=incremental_states, return_attn=return_attn, **decoder_kwargs))
        else:
            decoder_out = list(model.decoder(tokens, None, return_attn=return_attn, **decoder_kwargs))
        decoder_out[0] = decoder_out[0][:, -1:, :]
        if temperature != 1.:
            decoder_out[0].div_(temperature)
        attn = decoder_out[1]
        if type(attn) is dict:
            attn = attn['attn']
        # if attn is not None:
        #     if type(attn) is dict:
        #         attn = attn['attn']
        #     attn = attn[:, :, -1, :]  # B x L x t
        if return_logits:
            logits_t = decoder_out[0][:, -1, :]
            return logits_t, attn
        log_probs = model.get_normalized_probs(decoder_out, log_probs=True) # log softmax 
        log_probs = log_probs[:, -1, :]
        return log_probs, attn

    def _topk_decode(self, logits, topk, topp, return_prob=False):
        """WARNING!!! This can modify the `self.pad` position of `logits`."""
        if topk == 1 and topp == 0:  # greedy
            logits[:, self.pad] = -math.inf  # as in fairseq code
            pred_tok = logits.argmax(dim=1, keepdim=True)
        else:
            if topk > 1:
                logits[:, self.pad] = -1e10  # never select pad
                logits = top_k_logits(logits, topk)
                pred_tok = torch.softmax(logits, -1).multinomial(1)
            else:
                assert topp > 0.0
                filtered_probs, bookkeep_idx = self._sample_topp(torch.softmax(logits, 1), sampling_topp=topp)
                selected = filtered_probs.multinomial(1)
                pred_tok = torch.gather(bookkeep_idx, index=selected, dim=1)
        if return_prob:
            return pred_tok, torch.gather(torch.softmax(logits, -1), index=pred_tok, dim=1)
        return pred_tok
